# docker-compose.yml
version: "3.8"

services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870" # NameNode Web UI
      - "9000:9000" # FS Default
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - /home/iitgcpuser/iit/lab3_4_map_reduce/mapreduce-design-patterns/src/main/resources:/opt/hadoop/resources
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    restart: always
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    restart: always
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    depends_on:
      - namenode
      - datanode1
      - datanode2
    ports:
      - "8088:8088" # ResourceManager Web UI
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode1:9864 datanode2:9864"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager1
    restart: always
    depends_on:
      - resourcemanager
    environment:
      SERVICE_PRECONDITION: "resourcemanager:8088"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

  nodemanager2:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager2
    restart: always
    depends_on:
      - resourcemanager
    environment:
      SERVICE_PRECONDITION: "resourcemanager:8088"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop
  hive-postgres:
    image: postgres:9.6
    container_name: hive-postgres
    restart: always
    volumes:
      - hive_postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
      - POSTGRES_DB=metastore
    networks:
      - hadoop

  hive-metastore:
    image: bde2020/hive-metastore:2.3.2 # <-- This is the correct image
    container_name: hive-metastore
    restart: always
    depends_on:
      - hive-postgres
      - namenode
      - datanode1
      - datanode2
    ports:
      - "9083:9083" # Metastore Thrift port
    environment:
      SERVICE_PRECONDITION: "hive-postgres:5432 namenode:9000 namenode:9870"
      # These credentials are for this service to run schematool
      HIVE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-postgres:5432/metastore"
      HIVE_CONF_javax_jdo_option_ConnectionDriverName: "org.postgresql.Driver"
      HIVE_CONF_javax_jdo_option_ConnectionUserName: "hive"
      HIVE_CONF_javax_jdo_option_ConnectionPassword: "hive"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

  hive-server:
    image: bde2020/hive:2.3.2
    container_name: hive-server
    restart: always
    depends_on:
      - hive-metastore
      - resourcemanager
    ports:
      - "10000:10000" # HiveServer2 Thrift port (for Beeline/JDBC)
      - "10002:10002" # HiveServer2 Web UI
    environment:
      # This service waits for the metastore, which is now starting correctly
      SERVICE_PRECONDITION: "hive-metastore:9083"
      HIVE_CONF_hive_metastore_uris: "thrift://hive-metastore:9083"
      # NO database credentials here. This breaks the race condition.
    env_file:
      - ./hadoop.env
    networks:
      - hadoop

volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:
  hive_postgres_data: # <-- Add this line

networks:
  hadoop: